# Broadband DOA Estimation using CNN trained with noise signals阅读笔记
# 1. Introduction

Accurate DOA estimation is a challenging task in the presence of noise and reverberation.

Traditional methods (subspace based approaches MUSIC, GCC, SRP-PHAT, multichannel cross correlation coefficient MCCC, maximum likelihood method) suffer from problems such as high computational cost and/or degradation in performance in presence of noise and reverberation.

The phase component of STFT coefficients of the input signal is directly provided as input to the CNN, and the CNN learns the information required for DOA estimation during training. Using only the phase information also makes it possible to train the system with synthesized noise signals rather than real-world signals like speech. This makes the preparation of the training data set easier.

# 2. DOA Estimation As a Classification Problem

The aim of the CNN based framework is to learn a mapping from the observed microphone array signals to the DOA of the impinging sound wave using a large set of labeled training data.

![image-20210928214433551](https://tva1.sinaimg.cn/large/008i3skNly1guwoe3iywij60it0770u502.jpg)

# 3. CNN Based DOA Estimation

## 3.1. Input feature representation

In the STFT domain the observed signals at each TF instance are represented by complex numbers.

![image-20210929104626748](https://tva1.sinaimg.cn/large/008i3skNly1guxazmmkiuj60ix04zjrx02.jpg)

Rather than having an explicit feature extraction step, we directly provide the phase component of the STFT coefficients of the received signals as input to our system. The idea is to make the system learn the relevant feature for DOA estimation from the phase component through training.

第 n 帧的相位特征被称为 phase map，是一个大小为 $M \times K$ 的矩阵。

## 3.2. Convolutional neural networks - Basics

In typical CNN architectures, the convolution layers are pairs of convolution and pooling operation.



As each filter is applied across the whole input space, it leads to a critical concept in CNNs, called “weight sharing”, which leads to fewer trainable parameters compared to fully connected network.

## 3.3. DOA estimation with CNNs

The phase map for the n-th time frame as $\Phi_n$.

The posterior probability generated by the CNN at the output is given by $p(\theta_i|\Phi_n)$, where $\theta_i$ is the DOA corresponding to the i-th class.

![image-20210929140510753](https://tva1.sinaimg.cn/large/008i3skNly1guxgqfezg5j60m308q3z502.jpg)

In the convolution layers, small filters of size $2\times2$ are applied to learn local correlations between the phase components of neighboring microphones at local frequency regions.

In the presence of noise, the SNR across the spectrum is not constant, therefore the filters can detect local phase structures from the high SNR part well enough to compensate for the lack of information from the low SNR regions. Due to the weight sharing concept in CNNs, they also provide robustness to local distortions in the input. Therefore, applying the filters to learn local phase structure over neighboring microphones can provide additional robustness to small perturbations in microphone positions.

In contrast to conventional CNN architectures, we do not have any pooling layer. In our experiments, inclusion of pooling layers showed a slight decrease in performance.

Given the posterior probabilities, the final DOA estimate is given by
$$
\hat{\theta}_{n}=\underset{\theta_{i}}{\arg \max } p\left(\theta_{i} \mid \boldsymbol{\Phi}_{n}\right)
$$
Through various experiments with different sized networks, the architecture with the minimum average validation loss over data from different acoustic conditions was chosen as the final architecture.

The CNN is trained using a training data set $\{\{\Phi_n,\theta_n\} | n=1,...,N\}$, where $N$ denotes the total number of STFT time frames in the training set.

# 4. Training with Noise

Since the magnitude spectrum is not utilized, it is possible to prepare the training data set using synthesized signals rather than using actual speech recordings.

Advantages of being able to train the network with noise signals:

* do not require any speech databases for preparation of the training data set
* Makes the design of ground truth labels easier (VAD is required to detect silent frames when using speech signals). Errors in detecting silent frames can lead to inconsistent labels leading to error in training.

# 5. Experimental Results

## 5.1. CNN training

To form the classes, we discretize the whole DOA range of a ULA with a 5° resolution to get $I = 37$ DOA classes.

Loss function: cross-entropy

Optimizer: Adam gradient-based optimizer

Mini-batches: 512 time frames

Dropout: 0.5

![image-20210930150820031](https://tva1.sinaimg.cn/large/008i3skNly1guyo6g426aj60lp0f2mze02.jpg)

From the results it can be seen that for the unmatched conditions, the proposed method is still able to accurately localize the source for majority of the time frames, however the performance is slightly worse than the matched conditions scenario from the previous experiment.

![image-20210930151058705](https://tva1.sinaimg.cn/large/008i3skNly1guyo95sxd7j60le09ijsq02.jpg)

Fig 3 shows the probabilities generated by the two methods for a speech sample. The frame level probabilities were averaged over all active frames and normalized to 1.

## 5.4. Robustness to small perturbations in mic positions

SRP-PHAT requires exact knowledge of the array geometry for localization whereas for the proposed method, the perturbations lead to local distortions in the input phase map, which the CNN is robust against, due to the weight sharing concept.

## 5.5. Adaptability to real environments

![image-20211029085222170](https://tva1.sinaimg.cn/large/008i3skNly1gvvwa708ugj30li074js7.jpg)

8 mics improve the spatial selectivity for the SRP based method, thus SRP-PHAT performs better for lower reverberation times.



### Training data generation - Pseudo code

**Generate RIRs**

This pseudo-code explains the generation of RIRs for the different acoustic conditions. For the specific acoustic parameters used in this work, please refer to Table 1.

```python
Select R rooms of different sizes

for nb_room in range(1,R) 
    Randomly select P array positions
    Choose D source-array distances
    for nb_pos in range(1,P)
        for nb_dist in range(1,D)
            Generate RIRs corresponding to each of the 37 discrete DOAs and M microphones

Store the NR = R*P*D RIRs

NOTE: Each RIR file corresponds to a specific acoustic setup and contains 37 x M source-mic RIRs for each DOA and microphone in the array
```

In the referenced paper: R = 2, P = 7, D = 2

**Training data - Features and Target generation**

```python

for nb_rir in range(1,NR)
    for nb_ang in range(1,37)
        sig_anechoic = 2 s long white Gaussian noise  # each iteration a different variance was used
        sig_spatial = sig_anechoic convolved with the M RIRs
        sig_noisy = sig_spatial + noise  ## noise = spatially uncorrelated white noise with a randomly chosen SNR in the range of [0,20]dB
        
        sig_STFT = STFT(sig_noisy)   ## size M (mics) x K (frequency bins) x N (time frames)
        phase_component = angle(sig_STFT)
        
        for nb_frame in range(1,N)
            phase_map(nb_frame) = phase_component(:,:,nb_frame) # matrix of size M x K taken from phase_component
            target(nb_frame) = one-hot encoded vector of size 37 x 1 with the true DOA label as 1, rest 0s
        
# Training pairs
X_train = phase_map tensor of size M x K x 1 x (N*NR*37) # resizing done for input to Conv2D in Keras
Y_train = target matrix of size 37 x (N*NR*37)

NOTE: Since the SNRs for each nb_ang and nb_rir is randomly chosen, the whole procedure was repeated 
several times to have a balanced dataset in order to avoid a specific SNR bias. 
The size of the training data was influenced by the memory constraints.

```

