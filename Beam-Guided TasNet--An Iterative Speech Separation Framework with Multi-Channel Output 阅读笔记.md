#! https://zhuanlan.zhihu.com/p/565953373
# Beam-Guided TasNet: An Iterative Speech Separation Framework with Multi-Channel Output 阅读笔记

[[Code]](https://github.com/hangtingchen/Beam-Guided-TasNet)

# Abstract
Beam-TasNet links MC-Conv-TasNet with MVDR, which leverages the strong modeling ability of data-driven network and boosts the performance of BF with an accurate estimation of speech statistics.

MIMMO (multi-channel input, multi-channel multi-source output) speech separation system Beam-Guided TasNet, where MC-Conv-TasNet and MVDR can interact and promote each other more compactly under a directed cyclic flow.

The first stage uses Beam-TasNet to generate estimated single-speaker signals, which favors the separation in the second stage. The proposed framework facilitates iterative signal refinement with the guide of BF and seeks to reach the upper bound of the MVDR-based methods.

# 1. Intro
Since the considerable speech separation performance achieved by TasNet, BeamTasNet uses the estimated time-domain signals to compute the Spatial Correlation Matrices, which has outperformed the MVDR based on the oracle freq-domain masks.

Beam-Guided TasNet is a data-driven model guided by beamforming. The first stage uses a MC-ConvTasNet and MVDR BF to perform BSS. In the second stage, an MC-Conv-TasNet guided by MVDR-beamformed signals can refine separated signals iteratively.

Contributions:
1. The directed cyclic flow of the second stage promotes the MC-Conv-TasNet and MVDR iteratively and seeks to reach the upper bound of the MVDR-based methods.
2. The unfolding training further improves the performance.
3. A causal Beam-Guided TasNet is explored for online processing, illustrating that the Beam-Guided TasNet is effective even though the utt-level info is unreachable.

# 2. Beam-Guided TasNet
## 2.1. Beam-TasNet

${\{y_c\}}_{c}$: collection of $y_c$ along channels (c=1,...,C)

MC-Conv-TasNet generates $\hat{x}_{s,c}$ representing the estimated image of source $s$ on channel $c$.

Parallel Encode the input multi-channel signal into a 2-dimensional temporal-spectro representation $R_C$:
$R_c=\text{ParEnc}({\{y_c\}}_{c},c)$

Separator to estimate the temporal-spectro masks:
${\{\hat{M}_{s,c}\}}_{s}=\text{Separator}(R_c)$

Decoder to recover the single-speaker waveform:
${\hat{z}_{s,c}}=\text{Dec}(\hat{M}_{s,c}\odot R_c)$

The permutation solver determines the source order by comparing the similarity across channels with the output of the first channel. The MVDR accpets the reordered estimation and calculates the SCM for each source.

In summary, the Beam-TasNet uses MC-Conv-TasNet to estimate SCMs $\hat{\Phi}$ with the estimated multi-channel image signals ${\{\hat{z}_{s,c}\}}_{s,c}$ and uses MVDR to estimate $\hat{x}_{s,c}$, which can be formulated as
$$
{\{\hat{x}_{s,c}\}}_{s}=\text{Beam-TasNet}({\{y_c\}}_c,c)
$$
with each channel served as the reference channel and then do beamforming on the reference channel $c$.

## 2.2. MIMMO model
MC-Conv-TasNet uses different channel orders to obtain temporal-spectro representation for the reference channel (Eq. 2), e.g., $R_4$ for channel order [4,1,2,3]. To obtain estimated signal, MC-Conv-TasNet needs to be run $C$ times.

MC-Conv-TasNet's separator estimates the temporal-spectro masks for all channels and sources. Parallel decoders generate signals for different channels using different decoders. MC-Conv-TasNet only need to be run one time to get estimated signals for all sources and channels.

## 2.3. Beam-Guided TasNet
![](https://raw.githubusercontent.com/FYJNEVERFOLLOWS/Picture-Bed/main/202209/20220919144217.png)
As plotted in Fig. 1(a), the first stage in Beam-Guided TasNet employs the original Beam-TasNet. In the second stage, the network performs source separation additionally guided by the beamformed signal. The encoder of the MC-Conv-TasNet in the second stage accepts $(C+S\times C)$ channels, including $C$-channel mixtures and $S\times C$-speaker beamformed singals.

As shown in Fig. 1(b), we first feed the mixture signal $y_c$ through Beam-TasNet to obtain the enhanced single-speaker signals $\hat{x}_{s,c}$,
$$
{\{\hat{x}_{s,c}^{(1)}\}}_{s,c}=\text{Beam-TasNet}^{(1)}({\{y_c\}}_c)
$$

The second stage uses a second Beam-TasNet,
$$
\left\{\hat{x}_{s, c}^{(2: 1)}\right\}_{s, c}=\text { Beam-TasNet }{ }^{(2)}\left(\left\{y_c\right\}_c,\left\{\hat{x}_{s, c}^{(1)}\right\}_{s, c}\right)
$$
where superscript $\cdot^{(2: 1)}$ indicates that the signal is generated by the second stage in the first iteration.

**Different from target speaker extraction and neural spatial filtering, we deduce the source information by the enhanced signal calculated by the MVDR BF.**

The framework leads to a directed cyclic flow of multi-channel signals with iterative refinement implemented on the second stage (Fig. 1(a)). The second stage can iteratively accept $\hat{x}_{s, c}^{(2: n-1)}$ and generate $\hat{x}_{s, c}^{(2: n)}$,
$$
\left\{\hat{x}_{s, c}^{(2: n)}\right\}_{s, c}=\text { Beam-TasNet }^{(2)}\left(\left\{y_c\right\}_c,\left\{\hat{x}_{s, c}^{(2: n-1)}\right\}_{s, c}\right)
$$
where $n=2,3,...$ denotes the iteration number.

Unfold the second stage for SNR loss calculation in the training procedure:
$$
L=-\operatorname{SNR}\left(\hat{z}_{s, c}^{(1)}, x_{s, c}\right)-\operatorname{SNR}\left(\hat{z}_{s, c}^{(2: 1)}, x_{s, c}\right)-\operatorname{SNR}\left(\hat{z}_{s, c}^{(2: 2)}, x_{s, c}\right)
$$


## 2.5. Relation with other works
Deep unfolding: [Deep unfolding: Model-based inspiration of novel deep architectures]

Second-stage networks: Multi-microphone complex spectral mapping for utterance-wise and continuous speech separation
Computer-resource-aware deep speech separation with a run-time-specified number of BLSTM layers


# 3. Experimental setup
![](https://raw.githubusercontent.com/FYJNEVERFOLLOWS/Picture-Bed/main/202209/20220910201241.png)

![](https://raw.githubusercontent.com/FYJNEVERFOLLOWS/Picture-Bed/main/202209/20220910201327.png)

![](https://raw.githubusercontent.com/FYJNEVERFOLLOWS/Picture-Bed/main/202209/20220910201350.png)